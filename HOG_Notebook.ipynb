{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram of Oriented Gradients\n",
    "We explore the method of 'histogram of oriented gradients' (HOG) as described in Navneet Dalal and Bill Triggs paper entitled <u>Histograms of Oriented Gradients for Human Detection</u>. We will be using the INRIA Dataset, specifically those images found in 160x96 folder to test our implementation. The code to process the data we will be written in `Python`. In addition we will show how to use the HOG found in `OpenCV`.\n",
    "\n",
    "## Peope Detection\n",
    "To be able to detect people in an image we follow the approach outline in <u>Histograms of Oriented Gradients for Human Detection</u>. The idea presented in the paper is to capture the objects appearance and shape by characterizing it using local intensity gradients and edge directions. The image is densely divided into small spacial regions called cells. Each cell is then grouped into a block consisting of 4 cell arrange in a 2x2 configuration. The 160x96 pixel image divided into 19x11 blocks such that each block has a 50% overlap with another block. Within each of the blocks each cell is used to generate a 1-D histogram of gradient directions/edge directions and later all cell data is combined to give a complete histogram of oriented gradients descriptor of the window, resulting in a 19x11x(4*9) = 4930-D vector for a histogram utilizing 9 bins. The entire process is represented by the image below. \n",
    "\n",
    "<img src=\"HOG_Steps.png\" alt=\"\" style=\"width:auto; height:auto;\">\n",
    "\n",
    "(<i>In our case we will not be normalizing gamma nor we will be normalizing the colours, we will also be converting the images into grayscale before processing them further.</i>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# Standard Libraries\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Third Party Libraries\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradients\n",
    "The gradient is computed as a point discrete derivative mask. In their paper Dalal and Triggs describe various methods to compute the gradient and they discovered that using a 1-D $[−1, 0, 1]$ masks in both horizontal and vertical directions at $\\sigma=0$ work best. (The mask that were tested include an uncentred $[−1, 1]$, centred $[−1, 0, 1]$ and cubic corrected $[1, −8, 0, 8, −1]$, in addition to a 3×3 Sobel masks and 2×2 diagonal ones $\\left(\\begin{array}{rr}\n",
    "0 & 1 \\\\ \n",
    "-1 & 0 \\end{array} \\right)$ and $\\left(\\begin{array}{rr}\n",
    "-1 & 0 \\\\ \n",
    "0 & 1 \\end{array} \\right)$).\n",
    "\n",
    "Below we show how to generate the mask by defining a `grad` functions that takes the image as an argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grad(image):\n",
    "    grad_hor, grad_ver = np.zeros_like(image), np.zeros_like(image)\n",
    "\n",
    "    grad_hor = np.gradient(image, axis=1)\n",
    "    grad_hor[:, 1:-1] = (2 * grad_hor[:, 1:-1]) % 256\n",
    "\n",
    "    grad_ver = np.gradient(image, axis=0)\n",
    "    grad_ver[1:-1, :] = (2 * grad_ver[1:-1, :]) % 256\n",
    "\n",
    "    return grad_hor, grad_ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to computing the gradients we also need to compute the magnitudes and orientation. As per the findings in the paper we choose to go with an unsigned orientation thus limiting our angle to be between $0^{\\circ}$ and $180^{\\circ}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def magnitude_orientation(gx, gy):\n",
    "    magnitude = np.hypot(gx, gy)\n",
    "    orientation = (arctan2(gy, gx) * 180 / np.pi) % 180\n",
    "\n",
    "    return magnitude, orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weigthed Vote Into Spatial & Orientation Cells\n",
    "This stage involves dividing the window into adjacent, non-overlapping regions called cells of size $C\\times C$ pixel. For simplicity we choose to use rectangular cells, nevertheless, there is no restriction to the shape of a cell and Dalal and Triggs give a description of a radial (log-polar sectors) cell.  Within each cell \"each pixel calculates a weighted vote for an edge orientation histogram channel based on the orientation of the gradient element centred on it, and the votes are accumulated into orientation bins over local spatial regions. The orientation bins are evenly spaced over $0^{\\circ}$ to $180^{\\circ}$. To reduce aliasing, votes are interpolated bilinearly between the neighbouring bin centres in both orientation and position. The vote is a function of the gradient magnitude at the pixel, either the magnitude itself, its square, its square root, or a clipped form of the magnitude representing soft presence/absence of an edge at the pixel\".\n",
    "\n",
    "<b>Example: </b> \n",
    "To compute the weigthed votes let $B$ be the number of bins (in our case $B=9$), $m$ the magnitude, and $\\theta$ the orientation. We number the bins from $0$ to $B−1$ where each bin has width $w = \\frac{180}{B} \\big(\\frac{180}{9} = 20\\big)$. Bin $i$ has boundaries $[wi, w(i + 1))$ (in our case the boundary is $[20i, 20(i+1))$). The vote for a pixel is computed to be:\n",
    "\n",
    "<h5 align=\"center\">\n",
    "$\n",
    "\\begin{align}\n",
    "v_j &= m\\frac{w(j+1) − \\theta}{w} \\text{ to bin number } j \\\\\n",
    "v_{j+1} &= m\\frac{w(j+2) − \\theta}{w} \\ \\text{ to bin number } (j+1)\n",
    "\\end{align}\n",
    "$\n",
    "</h5>\n",
    "\n",
    "More concretely, if we let $\\theta = 77^{\\circ}$, $B=9$, $w=20$ then \n",
    "<h5 align=\"center\">\n",
    "$\n",
    "\\begin{align}\n",
    "v_3 &= m\\frac{w_4 − \\theta}{20} \\text{ to bin number } j = 3 \\\\\n",
    "&= m\\frac{80-77}{20} = m\\frac{3}{20} = 0.15m\\\\\n",
    "\\text{and}&\\\\\n",
    "v_{4} &= m\\frac{w_5 - \\theta}{20} \\ \\text{ to bin number } j =4 \\\\\n",
    "&= m\\frac{90 - 77}{20} = m\\frac{17}{20} = 0.85m\n",
    "\\end{align}\n",
    "$\n",
    "</h5>\n",
    "<img src=\"bilinear_inter2.png\" alt=\"\" style=\"width:auto; height:auto;\">\n",
    "\n",
    "To compute the \"weigthed vote into spatial & orientation cells\" we first need to build the cell which should take as arguments the orientation, the gradient, and the number of bins, this is done below in the `_build_cell` function. We can then define a `_build_hist` functions whih takes the same arguments as the `_build_cell` function in addition to cell size to build a histogram. We choose to compute the vote using the magnitude simalarly to the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_cell(orientation, gradient, nbins):\n",
    "\n",
    "    cell_histogram = np.zeros(shape=nbins)\n",
    "    b_step = 180/nbins\n",
    "\n",
    "    for orient, grad in zip(orientation, gradient):\n",
    "        for o, g in zip(orient, grad):\n",
    "            c_bin = np.int8(np.abs(o//20))\n",
    "            prop = np.abs(o % b_step) / b_step\n",
    "\n",
    "            if c_bin == (nbins -1):\n",
    "                cell_histogram[c_bin] += (1-prop) * g\n",
    "                cell_histogram[0] += prop * g\n",
    "            else:\n",
    "                cell_histogram[c_bin] += (1-prop) * g\n",
    "                cell_histogram[c_bin+1] += prop * g\n",
    "\n",
    "    return cell_histogram\n",
    "\n",
    "def _build_hist(orientation, gradient, nbins, cell_size):\n",
    "\n",
    "    cell_y, cell_x = cell_size    \n",
    "    overfill = tuple([s % c for s, c in zip(orientation.shape, cell_size)])\n",
    "\n",
    "    # Gets the left side of the image, if the image cannot be covered by\n",
    "    # equal sized cells\n",
    "    shape = tuple([s_i - over_i \n",
    "                   for s_i, over_i in zip(orientation.shape, overfill)])\n",
    "\n",
    "    histograms = np.zeros((shape[0]//cell_y, shape[1]//cell_x, nbins))\n",
    "    i = 0\n",
    "    for y in range(0, shape[0], cell_y):\n",
    "        j = 0\n",
    "        for x in range(0, shape[1], cell_x):\n",
    "            orient = orientation[y: y+cell_y, x: x+cell_x]\n",
    "            grad = gradient[y: y+cell_y, x:x+cell_x]\n",
    "            histograms[i, j, :] += _build_cell(orient, grad, nbins=nbins)\n",
    "            j += 1\n",
    "        i+= 1\n",
    "\n",
    "    return histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrast Normalize Over Overlapping Spatial Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient strengths vary over a wide range owing to local variations in illumination and foreground-background contrast, as such it is necessary to apply some form of normalization. Normalization is accomplished by grouping cells into overlapping blocks of $2\\times2$ cells, so that each block is of size size $2C×2C$ pixels. Two horizontally or vertically consecutive blocks overlap by two cells, that is, the block stride is C pixels. As a consequence, each internal cell is included into four seperate blocks. \n",
    "\n",
    "#### Normalization\n",
    "Dalal and Triggs evaluate four different normalization schemes. If we let $\\mathbf{v}$ be the unnormalized descriptor vector,\n",
    "$||v||_k$ be its k-norm for $k=1, 2$, and $\\varepsilon$ be a small constant then the normalizations are described as follows:\n",
    "\n",
    "<h5 align=\"center\">\n",
    "$\n",
    "\\begin{align}\n",
    "L_1 &= \\frac{v}{||v||_1 +\\varepsilon}\\\\\n",
    "L_{1,sqrt} &= \\sqrt{\\frac{v}{||v||_1 +\\varepsilon}}\\\\\n",
    "L_2 &= \\frac{v}{||v||_2 +\\varepsilon}\\\\\n",
    "L_{2,clip} &= L_2 \\text{ followed by clipping and renormalizing}\n",
    "\\end{align}\n",
    "$\n",
    "</h5>\n",
    "\n",
    "The authors claim that $L_{2,clip}, L_2$ and $L_{1,sqrt}$ all perform equally well, while simple $L_1$ reduces performance by 5%. As such we decide to use $L_2$ as a normalization scheme. \n",
    "\n",
    "The `_build_block` function defined bellow describes how to compute this stage using $L_2$ normalization, it takes the same arguments as `_build_hist` in addition to block_size. The function works by sliding over the orientation and gradients to compute the histograms of the cells and grouping them into appropriate size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_block(orientation, gradient, nbins, cell_size, block_size):\n",
    "\n",
    "    histograms = _build_hist(orientation, gradient, nbins, cell_size)\n",
    "    nbr_y, nbr_x, _ = histograms.shape\n",
    "    dim = reduce(lambda x, y: x*y, block_size)\n",
    "    hog_matrix = np.zeros(((nbr_y-1)*(nbr_x-1), dim*nbins))\n",
    "\n",
    "    i = 0 \n",
    "    for y in range(0, nbr_y-1):\n",
    "        j = 0\n",
    "\n",
    "        for x in range(0, nbr_x-1):\n",
    "            block = histograms[y: y+block_size[0], x: x+block_size[1]]\n",
    "            norm_value = np.linalg.norm(block.ravel() + 1e-4, ord=2)\n",
    "            block = block.ravel()/norm_value\n",
    "            position = (nbr_x-1)*i + j\n",
    "            hog_matrix[position] = block\n",
    "            j += 1\n",
    "        i+= 1\n",
    "\n",
    "    return hog_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG Descriptor\n",
    "Putting it all together we have the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hog(image, nbins=9, cell_size=(8, 8), block_size=(2, 2), flatten=True):\n",
    "    gx, gy = grad(image)\n",
    "    mag, orientation = magnitude_orientation(gx, gy)\n",
    "\n",
    "    hog_matrix = _build_block(orientation=orientation, gradient=mag, \n",
    "                              nbins=nbins, cell_size=cell_size, \n",
    "                              block_size=block_size)\n",
    "\n",
    "    if flatten:\n",
    "        return hog_matrix.ravel()\n",
    "    else:\n",
    "        return hog_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM\n",
    "The article describes using a linear SVM for training. We train the SVM using the sklearn library and we use K-fold cross validation to reduce the change of overfitting. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def hog_svm(data, seed=35, save=False, save_loc='C:\\\\...\\\\Train\\\\model.pkl'):\n",
    "\n",
    "\tfrom sklearn import svm\n",
    "\tfrom sklearn.model_selection import KFold\n",
    "\n",
    "\trng = np.random.RandomState(seed=seed)\n",
    "\trandom_index = rng.randint(0, data.shape[0], 4930)\n",
    "\tdata = data[random_index]\n",
    "\n",
    "\tkf = KFold(n_splits=10)\n",
    "\tclf = svm.SVC(kernel='linear')\n",
    "\tcoefficients = np.zeros(shape=(1, 7524))\n",
    "\n",
    "\tfor train, test in kf.split(data):\n",
    "\t\ttrain_data = data[train]\n",
    "\t\ttest_data = data[test]\n",
    "\n",
    "\t\tclf.fit(train_data[:, :-1], train_data[:, -1])\n",
    "\t\tpredictions = [int(a) for a in clf.predict(test_data[:, :-1])]\n",
    "\t\tnum_correct = sum(int(a==y) for a, y in zip(predictions, test_data[:, -1]))\n",
    "\n",
    "\tif save:\n",
    "\t\tfrom sklearn.externals import joblib\n",
    "\n",
    "\t\tprint(\"Saving Model\")\n",
    "\t\tjoblib.dump(clf, save_loc)\n",
    "        \n",
    "    return\n",
    "        clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People Detection Using a Sliding Window\n",
    "Finally, to be able to detect people in an image we use a sliding window approach which we define below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, window_size, step_size):\n",
    "\n",
    "    for y in range(0, image.shape[0], step_size):\n",
    "        for x in range(0, image.shape[1], step_size):\n",
    "            # yield the current window\n",
    "                img = image[y: y+window_size[1], x: x+window_size[0]]\n",
    "                if img.shape == (int(window_size[1]), int(window_size[0])):\n",
    "\n",
    "                    yield (x, y, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running The Code\n",
    "To run the code simply load the positive and negative training images, run `hog()` on each image to get the hog descriptor. Once the images have been processed simply call  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "loc_p = 'C:\\\\..\\\\Train\\\\pos\\\\'\n",
    "loc_n = 'C:\\\\...\\\\Train\\\\neg\\\\'\n",
    "\n",
    "p_files = os.listdir(loc_p)\n",
    "n_files = os.listdir(loc_n) \n",
    "total_files = len(p_files) + len(n_files)\n",
    "data = np.zeros(shape=(total_files, 7525))\n",
    "\n",
    "print(\"Starting to process postive training data\")\n",
    "for i, files in enumerate(p_files):\n",
    "    print(\"{}, {}\".format(i, files))\n",
    "    file = loc_p + str(files)\n",
    "    img = ndimage.imread(file, mode='L')\n",
    "    hog_descriptor = hog(img)\n",
    "\n",
    "    data[i, :-1] += hog_descriptor\n",
    "    data[i, -1] = -1\n",
    "\n",
    "print(\"\\nStarting to process negative training data\")\n",
    "for file in n_files:\n",
    "    i += 1\n",
    "    print(\"{}, {}\".format(i, file))\n",
    "    file = loc_n + str(file)\n",
    "    img = ndimage.imread(file, mode='L')\n",
    "    hog_descriptor = hog(img)\n",
    "\n",
    "    data[i, :-1] += hog_descriptor\n",
    "    data[i, -1] = 1\n",
    "    \n",
    "clf = hog_svm(data, save=False)\n",
    "\n",
    "## Will run sliding window over test image\n",
    "# test_im = 'C:\\\\...\\\\Train\\\\crop_000608.png'\n",
    "# img = ndimage.imread(test_im, mode='L')\n",
    "\n",
    "# rects = []\n",
    "# sl_window = sliding_window(img, window_size=(96, 160), step_size=8)\n",
    "# for window in sl_window:\n",
    "#    wdw = window[-1]\n",
    "#    hog_im = hog(wdw)\n",
    "#    hog_im = hog_im.reshape(1, -1)\n",
    "#    predict = clf.predict(hog_im)\n",
    "#    print(predict)\n",
    "\n",
    "#    if predict == -1:\n",
    "#         x, y = windows[0], window[1]\n",
    "#         rects.append([x, y, x+96, y+160])\n",
    " \n",
    "# pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve detection we can generate an image pyramid and apply a sliding window to each sub-image. We would then combine the overlapped boxes to produce our final detection. This can be done using a mean-shift algorithm to detect multiple modes in the bounding box space by utilizing the (x, y) coordinates of the bounding box as well as the logarithm of the current scale of the image as suggested by Dalal and Triggs. One can also use a non-maximum suppression algorithm to combine the overlapped boxes, we found that this yielded slightly better results  and can be found in the code utilizing `OpenCv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OpenCV\n",
    "The implimentation in OpenCV is more optimized then that presented above. Furthermore, `OpenCV` has a pretrained people detector, which facilitates the entire process. The code below shows how to use `OpenCV` to perform people detecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "box = 0\n",
    "\n",
    "loc = 'C:\\\\..\\\\Pedestrian_detection\\\\Datasets\\\\INRIA\\\\Train\\\\pos\\\\crop001001.png'\n",
    "im = cv2.imread(loc)\n",
    "im = imutils.resize(im, width=min(100, im.shape[1]))\n",
    "\n",
    "# detect people in the image\n",
    "rects, weights = hog.detectMultiScale(im, winStride=(1, 1),\n",
    "                                      padding=(6, 6), scale=1.05)\n",
    "    \n",
    "# for (x, y, w, h) in rects:\n",
    "#     cv2.rectangle(orig, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "# apply non-maxima suppression to the bounding boxes \n",
    "rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "\n",
    "# draw the final bounding boxes\n",
    "for (xA, yA, xB, yB) in pick:\n",
    "    cv2.rectangle(im, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "\n",
    "# show the output images\n",
    "cv2.imshow(\"People Detect\", im)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th><img src=\"People.png\" alt=\"Original\"></th>\n",
    "    <th><img src=\"people_detect.png\" alt=\"Original\"></th> \n",
    "  </tr>\n",
    " </table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
